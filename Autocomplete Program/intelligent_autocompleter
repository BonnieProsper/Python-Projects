#!/usr/bin/env python3
"""
Intelligent Autocomplete v4 

Features added in v4:
 - Reinforcement learning: accepting suggestion updates frequency & bigram counts
 - Smarter context: n-gram backoff (bigram -> unigram fallback)
 - POS tagging (optional via nltk) to bias suggestions by likely POS
 - Enhanced fuzzy: BK-tree caching and dynamic edit-distance tuning
 - Polished CLI: colorized output (optional), commands (/help, /stats, /learn ...)
 - Persistent storage of words, bigrams, stats, corrections
 - Unit-test friendly structure
"""

from __future__ import annotations
import json
import math
import os
import re
import sys
import time
from collections import defaultdict, Counter
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Iterable

# Optional packages
try:
    import colorama
    from colorama import Fore, Style
    colorama.init()
    COLOR = True
except Exception:
    COLOR = False

# Optional POS tagging
try:
    import nltk
    nltk.data.find("tokenizers/punkt")
    nltk.data.find("taggers/averaged_perceptron_tagger")
    HAS_NLTK = True
except Exception:
    HAS_NLTK = False

# ----------------------------
# Configuration
# ----------------------------
DATA_FILE = Path("autocomplete_v4_data.json")
DEFAULT_SUGGESTIONS = 8
RECENCY_CONSTANT = 3600.0       # recency boost (seconds)
EDIT_DISTANCE_WEIGHT = 1.4      # penalty per edit
CONTEXT_BOOST_WEIGHT = 2.0      # multiplier for context-based boost
FUZZY_CACHE_TTL = 300.0         # seconds to keep BK-tree query cache entries


# ----------------------------
# Utilities
# ----------------------------
def now() -> float:
    return time.time()


def lower(s: str) -> str:
    return s.strip().lower()


def color(text: str, c: str) -> str:
    if not COLOR:
        return text
    return f"{c}{text}{Style.RESET_ALL}"


# ----------------------------
# Levenshtein distance
# ----------------------------
def levenshtein(a: str, b: str) -> int:
    """Memory-efficient Levenshtein distance (two-row DP)."""
    if a == b:
        return 0
    if len(a) == 0:
        return len(b)
    if len(b) == 0:
        return len(a)
    if len(a) < len(b):
        a, b = b, a
    previous = list(range(len(b) + 1))
    for i, ca in enumerate(a, start=1):
        current = [i]
        for j, cb in enumerate(b, start=1):
            cost = 0 if ca == cb else 1
            current.append(min(previous[j] + 1, current[-1] + 1, previous[j - 1] + cost))
        previous = current
    return previous[-1]


# ----------------------------
# BK-tree for fuzzy matching (with caching)
# ----------------------------
class BKNode:
    __slots__ = ("word", "children")

    def __init__(self, word: str):
        self.word = word
        self.children: Dict[int, "BKNode"] = {}


class BKTree:
    def __init__(self):
        self.root: Optional[BKNode] = None
        self._cache: Dict[Tuple[str, int], Tuple[float, List[Tuple[str, int]]]] = {}

    def insert(self, word: str):
        word = lower(word)
        if self.root is None:
            self.root = BKNode(word)
            return
        node = self.root
        while True:
            d = levenshtein(word, node.word)
            child = node.children.get(d)
            if child is None:
                node.children[d] = BKNode(word)
                return
            node = child

    def build(self, words: Iterable[str]):
        self.root = None
        for w in words:
            self.insert(w)
        self._cache.clear()

    def query(self, target: str, max_dist: int) -> List[Tuple[str, int]]:
        target = lower(target)
        key = (target, max_dist)
        # cache with TTL
        entry = self._cache.get(key)
        if entry and now() - entry[0] < FUZZY_CACHE_TTL:
            return entry[1]

        results: List[Tuple[str, int]] = []

        def _rec(node: BKNode):
            d = levenshtein(target, node.word)
            if d <= max_dist:
                results.append((node.word, d))
            for edge, child in node.children.items():
                if (d - max_dist) <= edge <= (d + max_dist):
                    _rec(child)

        if self.root:
            _rec(self.root)
        results.sort(key=lambda x: (x[1], x[0]))
        self._cache[key] = (now(), results)
        return results


# ----------------------------
# Trie (vocabulary + basic metadata)
# ----------------------------
class TrieNode:
    __slots__ = ("children", "is_word", "freq", "last_used", "pos")
    def __init__(self):
        self.children: Dict[str, "TrieNode"] = {}
        self.is_word: bool = False
        self.freq: int = 0
        self.last_used: float = 0.0
        self.pos: Optional[str] = None  # optional POS tag (if available)


class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word: str, freq: int = 1, last_used: Optional[float] = None, pos: Optional[str] = None):
        w = lower(word)
        node = self.root
        for ch in w:
            node = node.children.setdefault(ch, TrieNode())
        node.is_word = True
        node.freq += freq
        node.last_used = last_used if last_used is not None else now()
        if pos:
            node.pos = pos

    def _find(self, prefix: str) -> Optional[TrieNode]:
        node = self.root
        for ch in lower(prefix):
            node = node.children.get(ch)
            if node is None:
                return None
        return node

    def iter_from(self, prefix: str) -> List[Tuple[str, TrieNode]]:
        start = self._find(prefix)
        if not start:
            return []
        out: List[Tuple[str, TrieNode]] = []
        stack: List[Tuple[str, TrieNode]] = [(prefix, start)]
        while stack:
            word_so_far, n = stack.pop()
            if n.is_word:
                out.append((word_so_far, n))
            for ch, child in n.children.items():
                stack.append((word_so_far + ch, child))
        return out

    def words_count(self) -> int:
        return len(self.iter_from(""))

    def remove(self, word: str) -> bool:
        path: List[Tuple[str, TrieNode]] = []
        node = self.root
        for ch in lower(word):
            if ch not in node.children:
                return False
            path.append((ch, node))
            node = node.children[ch]
        if not node.is_word:
            return False
        node.is_word = False
        node.freq = 0
        node.last_used = 0.0
        for ch, parent in reversed(path):
            child = parent.children[ch]
            if child.children or child.is_word:
                break
            del parent.children[ch]
        return True


# ----------------------------
# Bigram + Unigram model (context predictor)
# ----------------------------
class NGramModel:
    def __init__(self):
        self.unigrams: Counter = Counter()
        self.bigrams: Dict[str, Counter] = {}

    def add_sentence(self, sentence: str):
        words = re.findall(r"\w+", sentence.lower())
        for w in words:
            self.unigrams[w] += 1
        for a, b in zip(words, words[1:]):
            self.bigrams.setdefault(a, Counter())
            self.bigrams[a][b] += 1

    def top_next(self, prev: str, limit: int = 5) -> List[Tuple[str, int]]:
        prev = lower(prev)
        if prev in self.bigrams:
            return self.bigrams[prev].most_common(limit)
        # backoff to unigram top list
        return self.unigrams.most_common(limit)


# ----------------------------
# POS tagging helper (optional)
# ----------------------------
def pos_tag_word(word: str) -> Optional[str]:
    if not HAS_NLTK:
        return None
    try:
        tokens = nltk.word_tokenize(word)
        if not tokens:
            return None
        tag = nltk.pos_tag(tokens)[0][1]
        return tag
    except Exception:
        return None


# ----------------------------
# Candidate dataclass
# ----------------------------
@dataclass
class Candidate:
    word: str
    score: float
    freq: int
    last_used: float
    edit_dist: int
    context_boost: float
    pos: Optional[str] = None


# ----------------------------
# Smart engine
# ----------------------------
class SmartEngine:
    def __init__(self, trie: Trie, ngram: NGramModel, bk: BKTree):
        self.trie = trie
        self.ngram = ngram
        self.bk = bk

    def _recency_boost(self, last_used: float) -> float:
        if last_used <= 0:
            return 0.0
        elapsed = now() - last_used
        return max(0.0, RECENCY_CONSTANT / (elapsed + 1.0))

    def _context_boost(self, prev: Optional[str], candidate: str) -> float:
        if not prev:
            return 0.0
        top = self.ngram.top_next(prev, limit=20)
        if not top:
            return 0.0
        counts = {w: c for w, c in top}
        maxc = max(counts.values()) if counts else 1
        c = counts.get(candidate, 0)
        if c <= 0:
            return 0.0
        return CONTEXT_BOOST_WEIGHT * (c / maxc)

    def suggest(self, user_text: str, prev_context: Optional[str] = None, fuzzy: bool = True,
                limit: int = DEFAULT_SUGGESTIONS) -> List[Candidate]:
        text = lower(user_text)
        if not text:
            return []

        parts = text.split()
        prefix = parts[-1]
        prev = prev_context or (parts[-2] if len(parts) > 1 else None)

        candidates: Dict[str, Candidate] = {}

        # exact prefix matches
        for w, node in self.trie.iter_from(prefix):
            score = node.freq + self._recency_boost(node.last_used) + self._context_boost(prev, w)
            candidates[w] = Candidate(word=w, score=score, freq=node.freq,
                                      last_used=node.last_used, edit_dist=0,
                                      context_boost=self._context_boost(prev, w), pos=node.pos)

        # fuzzy matches (dynamic max dist)
        if fuzzy:
            if len(prefix) <= 4:
                maxd = 1
            elif len(prefix) <= 8:
                maxd = 2
            else:
                maxd = 3
            bk_results = self.bk.query(prefix, maxd)
            for w, d in bk_results:
                # skip if already present (we'll merge)
                node = self.trie._find(w)
                if node is None or not node.is_word:
                    continue
                base = node.freq + self._recency_boost(node.last_used)
                penalty = d * EDIT_DISTANCE_WEIGHT
                context = self._context_boost(prev, w)
                score = base + context - penalty
                existing = candidates.get(w)
                if existing:
                    existing.score = max(existing.score, score)
                    existing.edit_dist = min(existing.edit_dist, d)
                else:
                    candidates[w] = Candidate(word=w, score=score, freq=node.freq,
                                              last_used=node.last_used, edit_dist=d,
                                              context_boost=context, pos=node.pos)

        # Sort and return top candidates
        ranked = sorted(candidates.values(), key=lambda c: (-c.score, -c.freq, c.word))
        return ranked[:limit]


# ----------------------------
# Persistence
# ----------------------------
class Persistence:
    def __init__(self, path: Path = DATA_FILE):
        self.path = Path(path)

    def save(self, trie: Trie, ngram: NGramModel, stats: dict):
        words = []
        for w, node in trie.iter_from(""):
            words.append({"word": w, "freq": node.freq, "last_used": node.last_used, "pos": node.pos})
        bigr = []
        for prev, followers in ngram.bigrams.items():
            for nxt, cnt in followers.items():
                bigr.append({"prev": prev, "next": nxt, "count": cnt})
        data = {"words": words, "bigrams": bigr, "stats": stats, "saved_at": now()}
        self.path.parent.mkdir(parents=True, exist_ok=True)
        with self.path.open("w", encoding="utf-8") as fh:
            json.dump(data, fh, ensure_ascii=False, indent=2)

    def load(self) -> Tuple[Trie, NGramModel, dict]:
        trie = Trie()
        ngram = NGramModel()
        stats = {"accepts": 0, "adds": 0, "corrections": 0, "queries": 0}
        if not self.path.exists():
            return trie, ngram, stats
        try:
            with self.path.open("r", encoding="utf-8") as fh:
                data = json.load(fh)
            for item in data.get("words", []):
                trie.insert(item["word"], freq=int(item.get("freq", 1)),
                            last_used=float(item.get("last_used", 0.0)), pos=item.get("pos"))
            for b in data.get("bigrams", []):
                ngram.bigrams.setdefault(b["prev"], Counter())
                ngram.bigrams[b["prev"]][b["next"]] += int(b.get("count", 1))
            stats = data.get("stats", stats)
        except Exception:
            return Trie(), NGramModel(), stats
        return trie, ngram, stats


# ----------------------------
# CLI (polished)
# ----------------------------
class CLI:
    PROMPT = color("autocomplete> ", Fore.CYAN) if COLOR else "autocomplete> "

    def __init__(self, data_file: Path = DATA_FILE):
        self.persistence = Persistence(data_file)
        self.trie, self.ngram, self.stats = self.persistence.load()
        self.bk = BKTree()
        self._rebuild_bk()
        self.engine = SmartEngine(self.trie, self.ngram, self.bk)
        self.prev_context: Optional[str] = None
        self.fuzzy = True

    def _rebuild_bk(self):
        words = [w for w, _ in self.trie.iter_from("")]
        self.bk.build(words)

    def _print(self, *parts):
        if COLOR:
            print(" ".join(parts))
        else:
            print(" ".join(parts))

    def _help(self):
        text = [
            "/help                show this message",
            "/quit                save and exit",
            "/stats               show usage statistics",
            "/fuzzy on|off        toggle fuzzy matching",
            "/context <word|clear> set/clear previous-word context",
            "/learn <file>        learn sentences from a text file (one sentence per line)",
            "/remove <word>       remove a word from vocab",
            "/export <path>       export JSON dataset",
            "/import <path>       import JSON dataset",
        ]
        for line in text:
            print(line)

    def _stats(self):
        total = self.trie.words_count()
        print(f"vocab size: {total}")
        print(f"queries: {self.stats.get('queries',0)}, accepts: {self.stats.get('accepts',0)}, adds: {self.stats.get('adds',0)}, corrections: {self.stats.get('corrections',0)}")
        # top words
        top = sorted(((w, n.freq) for w, n in self.trie.iter_from("")), key=lambda x: -x[1])[:10]
        if top:
            print("top words:")
            for w, f in top:
                print(f"  {w} — {f}")

    def _learn_file(self, path: str):
        if not os.path.exists(path):
            print("file not found")
            return
        with open(path, "r", encoding="utf-8") as fh:
            count = 0
            for line in fh:
                s = line.strip()
                if not s:
                    continue
                self.learn_text(s)
                count += 1
        print(f"learned {count} sentences from {path}")

    def learn_text(self, sentence: str):
        # tokenise words and insert
        words = re.findall(r"\w+", sentence.lower())
        for w in words:
            p = None
            if HAS_NLTK:
                p = pos_tag_word(w)
            self.trie.insert(w, freq=1, last_used=now(), pos=p)
            self.bk.insert(w)
            self.stats["adds"] = self.stats.get("adds", 0) + 1
        self.ngram.add_sentence(sentence)
        self.persistence.save(self.trie, self.ngram, self.stats)

    def accept_suggestion(self, word: str):
        node = self.trie._find(word)
        if node is None or not node.is_word:
            self.trie.insert(word, freq=1, last_used=now(), pos=(pos_tag_word(word) if HAS_NLTK else None))
        else:
            node.freq += 1
            node.last_used = now()
        # bigram learning if context set
        if self.prev_context:
            self.ngram.bigrams.setdefault(self.prev_context, Counter())
            self.ngram.bigrams[self.prev_context][word] += 1
        self.stats["accepts"] = self.stats.get("accepts", 0) + 1
        self._rebuild_bk()
        self.persistence.save(self.trie, self.ngram, self.stats)

    def run(self):
        print(color("Intelligent Autocomplete v4 — type /help for commands", Fore.GREEN) if COLOR else "Intelligent Autocomplete v4 — type /help for commands")
        while True:
            try:
                inp = input(self.PROMPT).strip()
            except (KeyboardInterrupt, EOFError):
                print("\nExiting and saving.")
                self.persistence.save(self.trie, self.ngram, self.stats)
                return
            if not inp:
                continue
            if inp.startswith("/"):
                parts = inp.split(maxsplit=1)
                cmd = parts[0].lower()
                arg = parts[1].strip() if len(parts) > 1 else ""
                if cmd == "/help":
                    self._help()
                elif cmd == "/quit":
                    print("Saving...")
                    self.persistence.save(self.trie, self.ngram, self.stats)
                    return
                elif cmd == "/stats":
                    self._stats()
                elif cmd == "/fuzzy":
                    if arg.lower() in ("on", "true", "1"):
                        self.fuzzy = True
                        print("fuzzy enabled")
                    elif arg.lower() in ("off", "false", "0"):
                        self.fuzzy = False
                        print("fuzzy disabled")
                    else:
                        print("usage: /fuzzy on|off")
                elif cmd == "/context":
                    if not arg:
                        print("usage: /context <word|clear>")
                    elif arg.lower() == "clear":
                        self.prev_context = None
                        print("context cleared")
                    else:
                        self.prev_context = lower(arg)
                        print(f"context set to '{self.prev_context}'")
                elif cmd == "/learn":
                    if not arg:
                        print("usage: /learn <file>")
                    else:
                        self._learn_file(arg)
                elif cmd == "/remove":
                    if not arg:
                        print("usage: /remove <word>")
                    else:
                        ok = self.trie.remove(arg)
                        if ok:
                            self._rebuild_bk()
                            self.persistence.save(self.trie, self.ngram, self.stats)
                            print(f"removed {arg}")
                        else:
                            print("not found")
                elif cmd == "/export":
                    path = arg or "export_v4.json"
                    self.persistence.save(self.trie, self.ngram, self.stats)
                    os.replace(DATA_FILE, path) if DATA_FILE.exists() else print("no data to export")
                    print("exported")
                elif cmd == "/import":
                    if not arg or not os.path.exists(arg):
                        print("usage: /import <file>")
                    else:
                        # quick import json format expected
                        try:
                            with open(arg, "r", encoding="utf-8") as fh:
                                data = json.load(fh)
                            # import words
                            for w in data.get("words", []):
                                self.trie.insert(w["word"], freq=int(w.get("freq", 1)), last_used=float(w.get("last_used", now())))
                                self.bk.insert(w["word"])
                            # import bigrams
                            for b in data.get("bigrams", []):
                                self.ngram.bigrams.setdefault(b["prev"], Counter())
                                self.ngram.bigrams[b["prev"]][b["next"]] += int(b.get("count", 1))
                            self.persistence.save(self.trie, self.ngram, self.stats)
                            print("import complete")
                        except Exception as e:
                            print("import failed:", e)
                else:
                    print("unknown command, try /help")
                continue

            # normal interactive query: show suggestions and prompt to accept or type
            self.stats["queries"] = self.stats.get("queries", 0) + 1
            candidates = self.engine.suggest(inp, prev_context=self.prev_context, fuzzy=self.fuzzy, limit=DEFAULT_SUGGESTIONS)
            if not candidates:
                print("No suggestions.")
                # prompt to add
                add = input("Add entered words to vocabulary? (y/n): ").strip().lower()
                if add.startswith("y"):
                    self.learn_text(inp)
                continue

            # pretty print candidates
            print(color("Suggestions:", Fore.YELLOW) if COLOR else "Suggestions:")
            for i, c in enumerate(candidates, start=1):
                edit = f" edit={c.edit_dist}" if c.edit_dist else ""
                ctx = f" ctx={c.context_boost:.2f}" if c.context_boost else ""
                pos = f" pos={c.pos}" if c.pos else ""
                print(f" {i}. {c.word} (score={c.score:.2f}, freq={c.freq}{edit}{ctx}{pos})")
            print(" 0. [type full word or accept none]")

            choice = input("Choose [number / word / n]: ").strip()
            if not choice:
                continue
            if choice.isdigit():
                idx = int(choice)
                if idx == 0:
                    continue
                if 1 <= idx <= len(candidates):
                    chosen = candidates[idx - 1].word
                    self.accept_suggestion(chosen)
                    print(f"Accepted '{chosen}'")
                else:
                    print("invalid number")
            else:
                typed = lower(choice)
                # if exact candidate typed, accept
                if any(typed == c.word for c in candidates):
                    self.accept_suggestion(typed)
                    print(f"Accepted '{typed}'")
                else:
                    # treat as new phrase/word to learn
                    self.learn_text(choice)
                    print(f"Added '{choice}' to vocabulary")

# ----------------------------
# Entrypoint
# ----------------------------
def main():
    cli = CLI(DATA_FILE)
    cli.run()


if __name__ == "__main__":
    main()

